{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Classification_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = np.array(df.iloc[:26000, 1:])\n",
    "ytrain = np.array(df.iloc[:26000, 0])\n",
    "xtest = np.array(df.iloc[26001:, 1:].reset_index(drop=True))\n",
    "ytest = np.array(df.iloc[26001:, 0].reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = (xtrain - np.mean(xtrain, axis = 1).reshape(-1, 1))/np.std(xtrain, axis = 1).reshape(-1, 1)\n",
    "xtest = (xtest - np.mean(xtest, axis = 1).reshape(-1, 1))/np.std(xtest, axis = 1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MLP:\n",
    "#     def __init__(self,xin_size=len(xtrain[0]),yout_size=len(np.unique(ytrain)),Hidden_Layers=[10,20]):\n",
    "#         layers = Hidden_Layers+[yout_size]\n",
    "#         layers = [xin_size]+layers\n",
    "#         self.depth = len(layers)\n",
    "#         self.w = [np.random.rand(layers[i+1], layers[i]) for i in range(self.depth-1)]\n",
    "#         self.b = [np.random.rand(1, layers[i]) for i in range(1, self.depth)]\n",
    "#         self.z = []\n",
    "#         self.activ = []\n",
    "#         print(self.w[0].shape)\n",
    "#         print(self.b[0].shape)\n",
    "    \n",
    "#     def fn(self, z):\n",
    "#         return np.max(0, z)\n",
    "    \n",
    "    # def fprop(self, xinput: np.ndarray, softmax=True) -> np.ndarray:\n",
    "    #     z = xinput@self.w[0].T+np.tile(self.b[0],[len(xinput), 1])\n",
    "    #     output = np.maximum(0, z)\n",
    "    #     self.activ.append(output)\n",
    "    #     self.z.append(output)\n",
    "    #     for i in range(1, self.depth-1):\n",
    "    #         z = output@self.w[i].T+np.tile(self.b[i],[len(xinput), 1])\n",
    "    #         self.z.append(z)\n",
    "    #         output = np.maximum(0, z)\n",
    "    #         self.activ.append(output)\n",
    "    #     output = (output-np.mean(output, axis=1).reshape(-1, 1))/np.std(output, axis=1).reshape(-1, 1)\n",
    "    #     output = (np.exp(output))/np.sum(np.exp(output), axis=1).reshape(-1, 1)\n",
    "    #     return output\n",
    "    \n",
    "#     def bprop(self, xinput: np.ndarray, yinput: np.ndarray, softmax=True):\n",
    "#         m = len(xinput)\n",
    "#         x = self.fprop(xinput, softmax)\n",
    "#         x = x.reshape(len(xinput), -1, 1)\n",
    "#         y = yinput.reshape(-1, 1, 1)\n",
    "#         dw = []\n",
    "#         k = np.bool_(self.z[-1].reshape(m, 1, len(self.z[-1][0]))>0).astype(int)\n",
    "#         layer_deriv = (y/x+(1-y)/(1-x))*x*(1-x) #shape = (22000, 10, 1)\n",
    "#         layer_deriv = ((layer_deriv*(np.bool_(self.z[-1].reshape(len(self.z[-1]), len(self.z[-1][0]), 1)>0).astype(int))@(self.activ[-1].reshape(m, 1, -1)))@self.w[-1]).T*(np.bool_(self.z[-2].reshape(len(self.z[-2]), len(self.z[-2][0]), 1)>0).astype(int))\n",
    "#         # layer_deriv = layer_deriv@(np.bool_(self.z[-2].reshape(len(self.z[-2]), 1, len(self.z[-2][0]))>0).astype(int))\n",
    "#         # dw.append(layer_deriv*self.w[-1])\n",
    "#         # for i in range(len(self.w)):\n",
    "#         #     layer_deriv = layer_deriv@(np.bool_(self.z[-i].reshape(len(self.z[-i]), 1, len(self.z[-i][0]))>0).astype(int))\n",
    "#         #     # deriv = layer_deriv*self.w[-i]\n",
    "#         #     dw.append(layer_deriv)\n",
    "#         # last_layer_deriv = ((y/x+(1-y)/(1-x))*x*(1-x)*(np.bool_(self.activ[-1].reshape(len(self.activ[-1]), len((self.activ[-1])[0]), 1)>0).astype(int))).reshape(len(last_layer_deriv),len(last_layer_deriv[0]),1)\n",
    "#         # dw3 = last_layer_deriv*self.w[-1]\n",
    "#         # sec_last_layer_deriv = dw3*(np.bool_(x>0).astype(int))\n",
    "#         # dw2 = sec_last_layer_deriv@self.w[-2]\n",
    "#         return layer_deriv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(213)\n",
    "class MLP:\n",
    "    def __init__(self, xin_size=len(xtrain[0]), yout_size=len(np.unique(ytrain)), Hidden_Layers=[20, 20]):\n",
    "        layers = Hidden_Layers + [yout_size]\n",
    "        layers = [xin_size] + layers\n",
    "        self.depth = len(layers)\n",
    "        # Use He initialization for weights\n",
    "        self.w = [np.random.normal(0, np.sqrt(2 / (layers[i] + layers[i+1])), \n",
    "                                size=(layers[i+1], layers[i])) for i in range(self.depth - 1)]\n",
    "        self.b = [np.zeros((1, layers[i+1])) for i in range(self.depth - 1)]\n",
    "        self.z = []\n",
    "        self.activ = []\n",
    "\n",
    "    def fn(self, z):\n",
    "        return np.maximum(0, z)\n",
    "        #return 1/(1+np.exp(-z))\n",
    "    \n",
    "    def fn_prime(self, z):\n",
    "        return (z > 0).astype(int)\n",
    "        #return self.fn(z)*(1-self.fn(z))\n",
    "\n",
    "    def fprop(self, xinput: np.ndarray, softmax=True) -> np.ndarray:\n",
    "        self.z = []\n",
    "        self.activ = []\n",
    "        \n",
    "        # Forward propagation through hidden layers\n",
    "        output = xinput\n",
    "        for i in range(self.depth - 1):\n",
    "            z = output @ self.w[i].T + self.b[i]\n",
    "            output = self.fn(z)\n",
    "            self.z.append(z)\n",
    "            self.activ.append(output)\n",
    "        \n",
    "        # Softmax for final layer\n",
    "        output = np.exp(z) / np.sum(np.exp(z), axis=1).reshape(-1, 1)\n",
    "        return output\n",
    "\n",
    "    def bprop(self, xinput: np.ndarray, yinput: np.ndarray, softmax=True):\n",
    "        m = len(xinput)\n",
    "        x = self.fprop(xinput, softmax)\n",
    "        \n",
    "        delta = (x - yinput.reshape(-1, 1)) / m  # Softmax + Cross-Entropy Loss Gradient\n",
    "        \n",
    "        dw = []\n",
    "        db = []\n",
    "\n",
    "        for i in reversed(range(self.depth - 1)):\n",
    "            dw_i = delta.T @ (self.activ[i-1] if i > 0 else xinput)\n",
    "            db_i = np.sum(delta, axis=0, keepdims=True)\n",
    "            dw.insert(0, dw_i)\n",
    "            db.insert(0, db_i)\n",
    "            \n",
    "            if i > 0:\n",
    "                delta = (delta @ self.w[i]) * self.fn_prime(self.z[i-1])\n",
    "        \n",
    "        return dw, db\n",
    "    \n",
    "    def fit(self, xtrain, ytrain, lr=0.01, iterations=1000):\n",
    "        for i in range(iterations):\n",
    "            dw, db = self.bprop(xtrain, ytrain)\n",
    "            \n",
    "            # Update weights and biases\n",
    "            for j in range(self.depth - 1):\n",
    "                self.w[j] -= lr * dw[j]\n",
    "                self.b[j] -= lr * db[j]\n",
    "            \n",
    "            if i % 10 == 0:  # Print loss every 10 iterations\n",
    "                loss = -np.mean(np.sum(ytrain.reshape(-1, 1) * np.log(self.fprop(xtrain)), axis=1))\n",
    "                print(f'Iteration {i}, Loss: {loss}')\n",
    "    \n",
    "    def predict(self, xinput):\n",
    "        output = self.fprop(xinput)\n",
    "        return np.argmax(output, axis=1)\n",
    "    \n",
    "    def accuracy(self, xinput, yinput):\n",
    "        y_pred = self.predict(xinput)\n",
    "        # y_true = np.argmax(yinput, axis=1)\n",
    "        y_true = yinput\n",
    "        return np.mean(y_pred == y_true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 120.7670104264011\n",
      "Iteration 10, Loss: 120.74902505704023\n",
      "Iteration 20, Loss: 120.73431712743293\n",
      "Iteration 30, Loss: 120.72282694115918\n",
      "Iteration 40, Loss: 120.71438625146936\n",
      "Iteration 50, Loss: 120.708964292332\n",
      "Iteration 60, Loss: 120.70655683403915\n",
      "Iteration 70, Loss: 120.70713970274302\n",
      "Iteration 80, Loss: 120.71080547926647\n",
      "Iteration 90, Loss: 120.71749963488166\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m MLP(xin_size\u001b[38;5;241m=\u001b[39mxin_size, yout_size\u001b[38;5;241m=\u001b[39myout_size, Hidden_Layers \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m100\u001b[39m])\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Fit the model \u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mytrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.000006\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mw[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Check accuracy\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[75], line 61\u001b[0m, in \u001b[0;36mMLP.fit\u001b[1;34m(self, xtrain, ytrain, lr, iterations)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, xtrain, ytrain, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m):\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(iterations):\n\u001b[1;32m---> 61\u001b[0m         dw, db \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbprop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mytrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m         \u001b[38;5;66;03m# Update weights and biases\u001b[39;00m\n\u001b[0;32m     64\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepth \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n",
      "Cell \u001b[1;32mIn[75], line 41\u001b[0m, in \u001b[0;36mMLP.bprop\u001b[1;34m(self, xinput, yinput, softmax)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbprop\u001b[39m(\u001b[38;5;28mself\u001b[39m, xinput: np\u001b[38;5;241m.\u001b[39mndarray, yinput: np\u001b[38;5;241m.\u001b[39mndarray, softmax\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m     40\u001b[0m     m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(xinput)\n\u001b[1;32m---> 41\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfprop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxinput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msoftmax\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m     delta \u001b[38;5;241m=\u001b[39m (x \u001b[38;5;241m-\u001b[39m yinput\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m/\u001b[39m m  \u001b[38;5;66;03m# Softmax + Cross-Entropy Loss Gradient\u001b[39;00m\n\u001b[0;32m     45\u001b[0m     dw \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[1;32mIn[75], line 31\u001b[0m, in \u001b[0;36mMLP.fprop\u001b[1;34m(self, xinput, softmax)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepth \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     30\u001b[0m     z \u001b[38;5;241m=\u001b[39m output \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw[i]\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb[i]\n\u001b[1;32m---> 31\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz\u001b[38;5;241m.\u001b[39mappend(z)\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactiv\u001b[38;5;241m.\u001b[39mappend(output)\n",
      "Cell \u001b[1;32mIn[75], line 15\u001b[0m, in \u001b[0;36mMLP.fn\u001b[1;34m(self, z)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactiv \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn\u001b[39m(\u001b[38;5;28mself\u001b[39m, z):\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmaximum(\u001b[38;5;241m0\u001b[39m, z)\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m#return 1/(1+np.exp(-z))\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Assuming xtrain and ytrain are your training data and labels respectively\n",
    "xin_size = len(xtrain[0])\n",
    "yout_size = len(np.unique(ytrain))\n",
    "model = MLP(xin_size=xin_size, yout_size=yout_size, Hidden_Layers = [100, 100])\n",
    "\n",
    "# Fit the model \n",
    "model.fit(xtrain, ytrain, lr=0.000006, iterations=2000)\n",
    "model.w[0].shape\n",
    "\n",
    "# Check accuracy\n",
    "print('Training Accuracy:', model.accuracy(xtrain, ytrain))\n",
    "print('Testing Accuracy:', model.accuracy(xtest, ytest))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
